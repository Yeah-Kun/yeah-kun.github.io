<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="1. 单变量线性回归(Linear Regression with One Variable)1.1 模型表示$$h_\theta(x)=\theta_0+\theta_1x$$  像上述公式，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。  例子如下： 单变量线性方程，就是我们初中就学的一元一次函数。当然啦，除了这个模型之外，我们还有很多其他的线性模型，比如指数模型、对数模">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达机器学习课程笔记——第一周">
<meta property="og:url" content="http://yoursite.com/2017/05/11/Machine-Learning-the-first-week/index.html">
<meta property="og:site_name" content="Yeah Kun">
<meta property="og:description" content="1. 单变量线性回归(Linear Regression with One Variable)1.1 模型表示$$h_\theta(x)=\theta_0+\theta_1x$$  像上述公式，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。  例子如下： 单变量线性方程，就是我们初中就学的一元一次函数。当然啦，除了这个模型之外，我们还有很多其他的线性模型，比如指数模型、对数模">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%9B%9E%E5%BD%92%E5%9B%BE%E7%A4%BA.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%B9%B3%E6%96%B9%E8%AF%AF%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%B9%B3%E6%96%B9%E8%AF%AF%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%9B%BE%E7%A4%BA.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%9B%BE%E7%A4%BA1.1.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%9B%BE%E7%A4%BA2.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9B%BE%E7%A4%BA1.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9B%BE%E7%A4%BA2.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%90%8C%E6%AD%A5%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F1.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%CE%B1%E7%90%86%E8%A7%A3.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9B%B4%E8%A7%82%E5%9B%BE.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%B1%82%E5%81%8F%E5%AF%BC%E6%95%B0%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AE%9A%E4%B9%89.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E6%9F%90%E4%B8%AA%E7%9F%A9%E9%98%B5%E5%85%83%E7%B4%A0.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E4%B9%98%E9%99%A4%E6%B3%95.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E7%BB%84%E5%90%88%E8%BF%90%E7%AE%97.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%982.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%BA%94%E7%94%A8%E5%88%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E4%BE%8B.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AE%9A%E4%B9%89.png">
<meta property="og:image" content="http://opptp2jx7.bkt.clouddn.com/octave%E6%B1%82%E9%80%86%E7%9F%A9%E9%98%B5.png">
<meta property="og:updated_time" content="2017-05-11T13:44:24.231Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="吴恩达机器学习课程笔记——第一周">
<meta name="twitter:description" content="1. 单变量线性回归(Linear Regression with One Variable)1.1 模型表示$$h_\theta(x)=\theta_0+\theta_1x$$  像上述公式，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。  例子如下： 单变量线性方程，就是我们初中就学的一元一次函数。当然啦，除了这个模型之外，我们还有很多其他的线性模型，比如指数模型、对数模">
<meta name="twitter:image" content="http://opptp2jx7.bkt.clouddn.com/%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/05/11/Machine-Learning-the-first-week/"/>





  <title>吴恩达机器学习课程笔记——第一周 | Yeah Kun</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  















  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yeah Kun</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Yeah Kun blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/11/Machine-Learning-the-first-week/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yeah Kun">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yeah Kun">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">吴恩达机器学习课程笔记——第一周</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-11T12:10:20+08:00">
                2017-05-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="1-单变量线性回归-Linear-Regression-with-One-Variable"><a href="#1-单变量线性回归-Linear-Regression-with-One-Variable" class="headerlink" title="1. 单变量线性回归(Linear Regression with One Variable)"></a>1. 单变量线性回归(Linear Regression with One Variable)</h3><h4 id="1-1-模型表示"><a href="#1-1-模型表示" class="headerlink" title="1.1 模型表示"></a>1.1 模型表示</h4><p>$$h_\theta(x)=\theta_0+\theta_1x$$</p>
<blockquote>
<p>像上述公式，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>
</blockquote>
<p>例子如下：<br><img src="http://opptp2jx7.bkt.clouddn.com/%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png" alt="回归函数图示"></p>
<p>单变量线性方程，就是我们初中就学的一元一次函数。<br>当然啦，除了这个模型之外，我们还有很多其他的线性模型，比如指数模型、对数模型等等，除了线性模型之外，还有非线性模型，有这么多的模型，其目的就是在于更好的拟合训练集的数据，以使得预测率更高。</p>
<p>以下是对模型的具体定义：</p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%E5%9B%9E%E5%BD%92%E5%9B%BE%E7%A4%BA.png" alt="回归图示"></p>
<h3 id="2-代价函数-Cost-Function"><a href="#2-代价函数-Cost-Function" class="headerlink" title="2. 代价函数(Cost Function)"></a>2. 代价函数(Cost Function)</h3><blockquote>
<p>代价函数就是为了就是找到目的函数的最优解。</p>
</blockquote>
<p>因为在一个训练集中，有无数个模型（一元一次函数），我们需要找到最拟合这个训练集的一个函数，所以就引入了代价函数，用来找到那个最好的模型。</p>
<h4 id="2-1公式表示"><a href="#2-1公式表示" class="headerlink" title="2.1公式表示"></a>2.1公式表示</h4><p><img src="http://opptp2jx7.bkt.clouddn.com/%E5%B9%B3%E6%96%B9%E8%AF%AF%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png" alt="平方误差代价函数"></p>
<p>上述是平方误差代价函数，这也是常用到的代价函数，它通过目的函数跟各个实际值的误差平方建立新的函数。为了使这个值不受个别极端数据影响而产生巨大波动，采用类似方差再取二分之一的方式来减小个别数据的影响。<br><img src="http://opptp2jx7.bkt.clouddn.com/%E5%B9%B3%E6%96%B9%E8%AF%AF%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%9B%BE%E7%A4%BA.png" alt="平方误差代价函数图示"></p>
<h4 id="2-2-代价函数的直观理解①"><a href="#2-2-代价函数的直观理解①" class="headerlink" title="2.2 代价函数的直观理解①"></a>2.2 代价函数的直观理解①</h4><p>最优解即为代价函数的最小值，根据以上公式多次计算可得到代价函数的图像：<br><img src="http://opptp2jx7.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%9B%BE%E7%A4%BA1.1.png" alt="代价函数图示"><br>可以看到该代价函数的确有最小值，这里恰好是横坐标为1的时候。</p>
<h4 id="2-3-代价函数的直观理解②"><a href="#2-3-代价函数的直观理解②" class="headerlink" title="2.3 代价函数的直观理解②"></a>2.3 代价函数的直观理解②</h4><p>如果有更多参数，就会更为复杂，两个参数的时候就已经是三维图像了：<br><img src="http://opptp2jx7.bkt.clouddn.com/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%9B%BE%E7%A4%BA2.png" alt="代价函数图示2"></p>
<h3 id="3-梯度下降算法-Gradient-Descent"><a href="#3-梯度下降算法-Gradient-Descent" class="headerlink" title="3. 梯度下降算法(Gradient Descent)"></a>3. 梯度下降算法(Gradient Descent)</h3><blockquote>
<p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数J(θ0,θ1) 的最小值。</p>
</blockquote>
<p>个人理解，代价函数是分析模型与实际训练集之间的误差，而梯度下降算法的作用，就是找出那个误差最小的代价函数。</p>
<h4 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h4><p><img src="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3.png" alt="算法思想"></p>
<ul>
<li>从参数的某一个（组）值开始，比如从θ0=0和θ1=0开始</li>
<li>保持该（组）值持续减小，如果是一组值就要保证他们<strong>同步更新</strong>，直到找到我们希望找到的最小值</li>
</ul>
<p>我们要找到一条最快下山的路径，我们走的每一步大小就是α 。<br><img src="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9B%BE%E7%A4%BA1.png" alt="梯度下降图示1"></p>
<p>如果在不同的起点，最后到达的最低点也会不一样。<br><img src="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9B%BE%E7%A4%BA2.png" alt="梯度下降图示2"></p>
<h4 id="3-1批量梯度下降-batch-gradient-descent"><a href="#3-1批量梯度下降-batch-gradient-descent" class="headerlink" title="3.1批量梯度下降(batch gradient descent)"></a>3.1批量梯度下降(batch gradient descent)</h4><p>$$ repeat\  until\ convergence { \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) (for\ j=0\ and\ j=1) } $$</p>
<ul>
<li>α：学习速率，决定我们让代价函数下降程度最大的方向迈出的步子有多大</li>
</ul>
<h5 id="3-1-1-同步更新-Simultaneous-update"><a href="#3-1-1-同步更新-Simultaneous-update" class="headerlink" title="3.1.1 同步更新(Simultaneous update)"></a>3.1.1 同步更新(Simultaneous update)</h5><p>在梯度下降算法中，我们需要更新θ0,θ1，实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要<strong>同时</strong>更新。</p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%E5%90%8C%E6%AD%A5%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F1.png" alt="同步更新公式"></p>
<h5 id="3-1-2-梯度下降算法理解"><a href="#3-1-2-梯度下降算法理解" class="headerlink" title="3.1.2 梯度下降算法理解"></a>3.1.2 梯度下降算法理解</h5><p>如果 α 太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果 α 太大，它会导致无法收敛，甚至发散。</p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%CE%B1%E7%90%86%E8%A7%A3.png" alt="对α的理解"></p>
<h5 id="解决方法——乘偏导数"><a href="#解决方法——乘偏导数" class="headerlink" title="解决方法——乘偏导数"></a>解决方法——乘偏导数</h5><p><img src="http://opptp2jx7.bkt.clouddn.com/%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9B%B4%E8%A7%82%E5%9B%BE.png" alt="批量梯度下降直观图"></p>
<p>首先初始化我的梯度下降算法，在那个品红色的点初始化，如果<br>我更新一步梯度下降，随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所 以，我再进行一步梯度下降时，我的导数项是更小的，θ1更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。</p>
<h5 id="3-1-3-线性回归的批量梯度下降"><a href="#3-1-3-线性回归的批量梯度下降" class="headerlink" title="3.1.3 线性回归的批量梯度下降"></a>3.1.3 线性回归的批量梯度下降</h5><p>偏导数求解推导过程</p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%E6%B1%82%E5%81%8F%E5%AF%BC%E6%95%B0%E8%BF%87%E7%A8%8B.png" alt="偏导数求解推导过程"></p>
<h5 id="批量梯度下降方程"><a href="#批量梯度下降方程" class="headerlink" title="批量梯度下降方程"></a>批量梯度下降方程</h5><p>通过上面几条公式的整合，最终得出以下公式<br><img src="http://opptp2jx7.bkt.clouddn.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png" alt="线性回归方程"></p>
<h4 id="4-线性代数基础"><a href="#4-线性代数基础" class="headerlink" title="4. 线性代数基础"></a>4. 线性代数基础</h4><p>个人现在认为，线性代数的作用主要是为了方便操作训练集。</p>
<h5 id="4-1-矩阵的定义"><a href="#4-1-矩阵的定义" class="headerlink" title="4.1 矩阵的定义"></a>4.1 矩阵的定义</h5><p>横为行，竖为列，表示方法一般是$ R^{m*n} $<br><img src="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AE%9A%E4%B9%89.png" alt="矩阵的定义"></p>
<p>寻找某个矩阵元素<br><img src="http://opptp2jx7.bkt.clouddn.com/%E6%9F%90%E4%B8%AA%E7%9F%A9%E9%98%B5%E5%85%83%E7%B4%A0.png" alt="某个矩阵元素"></p>
<h5 id="4-2-矩阵加法-Matrix-Addition"><a href="#4-2-矩阵加法-Matrix-Addition" class="headerlink" title="4.2 矩阵加法(Matrix Addition)"></a>4.2 矩阵加法(Matrix Addition)</h5><p>同一个位置的矩阵元素相加，得到新的矩阵<br><img src="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95.png" alt="矩阵加法"></p>
<h5 id="4-3-矩阵乘法-Scalar-Multiplication"><a href="#4-3-矩阵乘法-Scalar-Multiplication" class="headerlink" title="4.3 矩阵乘法(Scalar Multiplication)"></a>4.3 矩阵乘法(Scalar Multiplication)</h5><p>将值与矩阵每个元素相乘，得到新的矩阵<br><img src="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E4%B9%98%E9%99%A4%E6%B3%95.png" alt="矩阵乘法"></p>
<h5 id="4-4-矩阵的组合运算-Combination-of-Operands"><a href="#4-4-矩阵的组合运算-Combination-of-Operands" class="headerlink" title="4.4 矩阵的组合运算(Combination of Operands)"></a>4.4 矩阵的组合运算(Combination of Operands)</h5><p>将矩阵加减法和乘除法结合起来，道理都一样<br><img src="http://opptp2jx7.bkt.clouddn.com/%E7%9F%A9%E9%98%B5%E7%BB%84%E5%90%88%E8%BF%90%E7%AE%97.png" alt="矩阵的组合运算"></p>
<h5 id="4-5-两个矩阵相乘"><a href="#4-5-两个矩阵相乘" class="headerlink" title="4.5 两个矩阵相乘"></a>4.5 两个矩阵相乘</h5><p>A矩阵的行 乘 B矩阵的列 得到新矩阵 y 。<br><img src="http://opptp2jx7.bkt.clouddn.com/%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98.png" alt="两个矩阵相乘1"></p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%982.png" alt="两个矩阵相乘2"></p>
<h5 id="4-6-矩阵应用到梯度下降算法实例"><a href="#4-6-矩阵应用到梯度下降算法实例" class="headerlink" title="4.6 矩阵应用到梯度下降算法实例"></a>4.6 矩阵应用到梯度下降算法实例</h5><p>把训练集做成一个矩阵，把线性回归方程做成另外一个矩阵，将两个矩阵相乘，最后就能得出一个新的矩阵。<br><img src="http://opptp2jx7.bkt.clouddn.com/%E5%BA%94%E7%94%A8%E5%88%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E4%BE%8B.png" alt="矩阵应用到梯度下降算法实例图示"></p>
<h5 id="4-7-单位矩阵"><a href="#4-7-单位矩阵" class="headerlink" title="4.7 单位矩阵"></a>4.7 单位矩阵</h5><blockquote>
<p>在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,这种矩阵被称为单位矩阵．它是个方阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1。除此以外全都为0。</p>
</blockquote>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5.png" alt="单位矩阵"></p>
<p>除0矩阵外，任何矩阵乘单位矩阵都等于它本身。</p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97.png" alt="单位矩阵运算"></p>
<h5 id="4-8-逆矩阵"><a href="#4-8-逆矩阵" class="headerlink" title="4.8 逆矩阵"></a>4.8 逆矩阵</h5><p><img src="http://opptp2jx7.bkt.clouddn.com/%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AE%9A%E4%B9%89.png" alt="定义"></p>
<p>用octave求得逆矩阵：pinv()函数</p>
<p><img src="http://opptp2jx7.bkt.clouddn.com/octave%E6%B1%82%E9%80%86%E7%9F%A9%E9%98%B5.png" alt="octave求得逆矩阵"></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/03/简单Python刷票脚本/" rel="next" title="简单Python刷票脚本">
                <i class="fa fa-chevron-left"></i> 简单Python刷票脚本
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/06/嵌入式Linux课/" rel="prev" title="嵌入式Linux课">
                嵌入式Linux课 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Yeah Kun" />
          <p class="site-author-name" itemprop="name">Yeah Kun</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-单变量线性回归-Linear-Regression-with-One-Variable"><span class="nav-number">1.</span> <span class="nav-text">1. 单变量线性回归(Linear Regression with One Variable)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-模型表示"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 模型表示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-代价函数-Cost-Function"><span class="nav-number">2.</span> <span class="nav-text">2. 代价函数(Cost Function)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1公式表示"><span class="nav-number">2.1.</span> <span class="nav-text">2.1公式表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-代价函数的直观理解①"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 代价函数的直观理解①</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-代价函数的直观理解②"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 代价函数的直观理解②</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-梯度下降算法-Gradient-Descent"><span class="nav-number">3.</span> <span class="nav-text">3. 梯度下降算法(Gradient Descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法思想"><span class="nav-number">3.1.</span> <span class="nav-text">算法思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1批量梯度下降-batch-gradient-descent"><span class="nav-number">3.2.</span> <span class="nav-text">3.1批量梯度下降(batch gradient descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-1-同步更新-Simultaneous-update"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.1.1 同步更新(Simultaneous update)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-2-梯度下降算法理解"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.1.2 梯度下降算法理解</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解决方法——乘偏导数"><span class="nav-number">3.2.3.</span> <span class="nav-text">解决方法——乘偏导数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-3-线性回归的批量梯度下降"><span class="nav-number">3.2.4.</span> <span class="nav-text">3.1.3 线性回归的批量梯度下降</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#批量梯度下降方程"><span class="nav-number">3.2.5.</span> <span class="nav-text">批量梯度下降方程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-线性代数基础"><span class="nav-number">3.3.</span> <span class="nav-text">4. 线性代数基础</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-矩阵的定义"><span class="nav-number">3.3.1.</span> <span class="nav-text">4.1 矩阵的定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-矩阵加法-Matrix-Addition"><span class="nav-number">3.3.2.</span> <span class="nav-text">4.2 矩阵加法(Matrix Addition)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-3-矩阵乘法-Scalar-Multiplication"><span class="nav-number">3.3.3.</span> <span class="nav-text">4.3 矩阵乘法(Scalar Multiplication)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-4-矩阵的组合运算-Combination-of-Operands"><span class="nav-number">3.3.4.</span> <span class="nav-text">4.4 矩阵的组合运算(Combination of Operands)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-5-两个矩阵相乘"><span class="nav-number">3.3.5.</span> <span class="nav-text">4.5 两个矩阵相乘</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-6-矩阵应用到梯度下降算法实例"><span class="nav-number">3.3.6.</span> <span class="nav-text">4.6 矩阵应用到梯度下降算法实例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-7-单位矩阵"><span class="nav-number">3.3.7.</span> <span class="nav-text">4.7 单位矩阵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-8-逆矩阵"><span class="nav-number">3.3.8.</span> <span class="nav-text">4.8 逆矩阵</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yeah Kun</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  

  

  

  

</body>
</html>
